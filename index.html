<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Minjoon Jung</title>

    <meta name="author" content="Minjoon Jung">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
    <!--    <link rel="icon" type="image/png" href="images/logo.pdf]" style="width:150%;max-width:150%">-->

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        /* Style the container */
        .scrollable-container {
            width: 100%;
            height: 100px;
            overflow: scroll;
            background-color: white;
            padding: 1px;
            border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;
        }

        /* Custom scrollbar styles */
        .scrollable-container::-webkit-scrollbar {
            width: 5px; /* Width of the scrollbar */
        }

        .scrollable-container::-webkit-scrollbar-track {
            background: white; /* Light gray track */
        }

        .scrollable-container::-webkit-scrollbar-thumb {
            background: #888; /* Darker gray thumb */
            border-radius: 6px;
        }

        .scrollable-container::-webkit-scrollbar-thumb:hover {
            background: #555; /* Darker thumb when hovered */
        }
        .scrollable-container {
            height: 150px; /* adjust height as needed */
            width: 90%;   /* or set a fixed width like 400px */
            overflow-y: auto;
            border: 1px solid #ccc;
            padding: 10px;
        }
        .new-badge {
            background-color: red;
            color: white;
            font-size: 0.7em;
            padding: 2px 6px;
            border-radius: 5px;
            margin-left: 1px;
            animation: blink 2s ease-in infinite;
        }

        @keyframes blink {
            from, to { opacity: 1 }
            50% { opacity: 0 }
        }

        .scrollable-container ul {
            list-style-type: none;
            padding: 0;
        }

        .scrollable-container li {
            margin-bottom: 10px;
        }
        .button {
            padding: 5px 10px;
            margin: 5px;
            cursor: pointer;
            border: 0.5px solid #ccc;
            background-color: #f0f0f0;
            display: inline-block;
        }

        .button.active {
            background-color: #007BFF;
            color: white;
        }

        .tab-content {
            display: none;
        }

        .tab-content.active {
            display: block;
        }
    </style>
    <script>
        function showTab(tabId) {
            // Hide all tab contents
            var allTabs = document.querySelectorAll('.tab-content');
            allTabs.forEach(function(tab) {
                tab.classList.remove('active');
            });

            // Remove active class from all buttons
            var buttons = document.querySelectorAll('.button');
            buttons.forEach(function(button) {
                button.classList.remove('active');
            });

            // Show the selected tab content and add active class to the corresponding button
            document.querySelector(tabId).classList.add('active');
            var activeButton = document.querySelector(`.button[data-ref="${tabId}"]`);
            activeButton.classList.add('active');
        }
    </script>
</head>

<body>
<table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<tr style="padding:0px">
    <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                    <name>Minjoon Jung</name>
                </p>
                <p> Hi there! My name is Minjoon Jung, and I'm a Ph.D. student at <a href="https://en.snu.ac.kr/">Seoul National University</a>.
                    I have a broad interest in AI systems that interact with <span class="highlight" style="font-weight: bold;">multimodal data (Language + X, where X includes vision, video, and robotics)</span>.
                    My long-term research goal is to develop fine-grained, trustworthy multimodal AI agents that are both interpretable and highly efficient.
                    Previously, I interned at <a href="https://cvml.comp.nus.edu.sg/">NUS@CVML</a>, where I had the great opportunity to conduct research with <a href="https://doc-doc.github.io/cv/">Dr. Junbin Xiao</a> and <a href="https://www.comp.nus.edu.sg/cs/people/ayao/">Prof. Angela Yao</a>.
                </p>
                <p style="color: #f09228; font-weight: bold">
                     I'm actively seeking research internship and collaboration opportunities. Feel free to reach out if you would like to discuss!
                </p>
                <p>
                </p>
                <p style="text-align:center">
                    <a href="mailto:minjoon507@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.co.kr/citations?user=YORj6_YAAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/minjoong507/">Github</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/minjoonjung/">Linkedin</a>
                    <!--                <a href="https://instagram.com/mjjxxng_?igshid=OGQ5ZDc2ODk2ZA==">Instagram</a>-->
                </p>
            </td>
            <td style="padding:2.5%;width:20%;max-width:20%">
                <a href="images/profile2.jpg"><img style="width:130%;max-width:130%" alt="profile photo" src="images/profile2.JPG" class="hoverZoomLink"></a>
            </td>
        </tr>
        </tbody></table>

        <!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
        <!--        <tr>-->
        <!--            <td style="padding:20px;width:100%;vertical-align:middle">-->
        <!--                <heading>Research</heading>-->
        <!--                <p>-->
        <!--                    I have a broad interest in <span class="highlight">video-modeling</span> and <span class="highlight">trustworthy video comprehension</span>.-->
        <!--                    These days, I have been working on video large language models for fine-grained video understanding.-->
        <!--                </p>-->
        <!--            </td>-->
        <!--        </tr>-->
        <!--        </tbody></table>-->
        <!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
            </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <body>
<!--        <span class="new-badge">New</span></li>-->
        <div class="scrollable-container">
            <ul>
                <li>[2025.08] One paper has been accepted by EMNLP 2025! <span class="new-badge">New</span></li>
                <li>[2025.04] One paper has been accepted by ICLR 2025 Workshop!</li>
                <li>[2025.02] One paper has been accepted by CVPR 2025!</li>
                <li>[2024.11] One paper has been accepted by NeurIPS 2024 Workshop! We also released the extended version on arXiv.</li>
                <li>[2024.08] One paper has been early accepted by WACV 2025 Round 1!</li>
                <li>[2024.02] One paper has been accepted by IROS 2024!</li>
                <li>[2024.02] I'll be joining at National University of Singapore as a research intern.</li>
                <li>[2022.10] One paper has been accepted by EMNLP 2022!</li>
            </ul>
        </div>

        </body>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Publications&nbsp;&nbsp;</heading>
                <div class="button active" data-ref="#papers-selected" onclick="showTab('#papers-selected')">Show Selected</div>
                <div class="button" data-ref="#papers-all" onclick="showTab('#papers-all')">Show by Date</div>
                <!--                <hr style="width:100%;border:0;background-color:#BBBBBB;height:0.7px;">-->
            </td>
        </tr>
        </tbody>
        </table>
        <!-- Selected Papers -->
        <div id="papers-selected" class="tab-content active">
            <!--            <heading> Selected Publications&nbsp;&nbsp;</heading>-->

            <div style="display: flex; align-items: flex-start; gap: 20px; padding: 20px;">
                <!-- Image Container -->

                <div class="image-container" style="flex: 0 0 250px; display: flex; justify-content: center;">
                    <img src="images/egoexo.png" width="320" height="220">
                </div>

                <!-- Text Content Container -->
                <div class="text-container" style="flex: 1; max-width: 600px;">
                    <p class="papertitle" style="font-weight: bold; font-size: 1.0em;">
                        Towards Video Large Langauge Models for View-Invariant Video Understanding
                    </p>
                    <p>
                        <u>Minjoon Jung</u>,
                        <a href="https://doc-doc.github.io/cv/">Junbin Xiao*</a>,
                        <a href="https://jhkim-snu.github.io/">Junghyun Kim</a>,
                        Il-Jae Kwon,
                        <a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang</a>,
                        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a>
                    </p>
                    <p style="font-style: italic;">
                        Technical Report, 2025 <br>
                        </span>
                        <br>
                        TBD
                        <!--                        <a href="https://arxiv.org/abs/2411.12951">paper</a> /-->
                        <!--                        <a href="https://github.com/minjoong507/Consistency-of-Video-LLM">code</a>-->
                    </p>
                    <p style="font-size: 12px">
                        We introduce EgoExo-Con, a new benchmark comprising synchronized ego-exo video with human-refined queries, to study view-invariant video understanding in Video-LLMs. We also propose a scalable self-guided learning method that achieves consistent temporal comprehension with comparable performance to state-of-the-art models.
                    </p>
                </div>
            </div>

            <div style="display: flex; align-items: flex-start; gap: 20px; padding: 20px;">
                <!-- Image Container -->
                <div class="image-container" style="flex: 0 0 250px; display: flex; justify-content: center;">
                    <img src="images/consistency.gif" width="320" height="180" alt="Animated GIF">
                </div>

                <!-- Text Content Container -->
                <div class="text-container" style="flex: 1; max-width: 600px;">
                    <p class="papertitle" style="font-weight: bold; font-size: 1.0em;">
                        On the Consistency of Video Large Language Models in Temporal Comprehension
                    </p>
                    <p>
                        <u>Minjoon Jung</u>,
                        <a href="https://doc-doc.github.io/cv/">Junbin Xiao*</a>,
                        <a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang</a>,
                        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a>
                    </p>
                    <p style="font-style: italic;">
                        Conference on Computer Vision and Pattern Recognition (CVPR), 2025 <br>
                        <span style="font-size: smaller;">
                      *Earlier version has been accepted by
                      <a href="https://neurips.cc/virtual/2024/103565" style="font-size: inherit;">NeurIPS 2024 Workshop on Video-Language Models</a>.
                    </span>
                        <br>
                        <a href="https://arxiv.org/abs/2411.12951">paper</a> /
                        <a href="https://github.com/minjoong507/Consistency-of-Video-LLM">code</a>
                    </p>
                    <p style="font-size: 12px">
                        We reveal that Video Large Language Models (Video-LLMs) struggle to maintain consistency in grounding and verification. We systematically analyze this issue and introduce VTune, an effective instruction tuning method, leading to substantial improvements in both grounding and consistency.
                    </p>
                </div>
            </div>

            <div style="display: flex; align-items: flex-start; gap: 20px; padding: 20px;">
                <!-- Image Container -->
                <div class="image-container" style="flex: 0 0 250px; display: flex; justify-content: center;">
                    <img src="images/bm-detr.png" width="320" height="160" alt="Image">
                </div>

                <!-- Text Content Container -->
                <div class="text-container" style="flex: 1; max-width: 600px;">
                    <p class="papertitle" style="font-weight: bold; font-size: 1.0em;">
                        Background-aware Moment Detection for Video Moment Retrieval
                    </p>
                    <p>
                        <u>Minjoon Jung</u>,
                        Youwon Jang,
                        Seongho Choi,
                        <a href="https://tikatoka.github.io/">Joochan Kim</a>,
                        <a href="http://wityworks.com/">Jin-Hwa Kim*</a>,
                        <a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang*</a>
                    </p>
                    <p style="font-style: italic;">
                        Winter Conference on Applications of Computer Vision (WACV), 2025 <br>
                        <span style="font-size: smaller;">*Early accepted in Round 1.</span> <br>
                        <a href="https://arxiv.org/abs/2306.02728">paper</a> /
                        <a href="https://github.com/minjoong507/BM-DETR">code</a>
                    </p>
                    <p style="font-size: 12px">
                        We propose Background-aware Moment Detection TRansformer (BM-DETR), which carefully adopts a contrastive approach for robust prediction. BM-DETR achieves state-of-the-art performance on various benchmarks while being highly efficient.
                    </p>
                </div>
            </div>

            <div style="display: flex; align-items: flex-start; gap: 20px; padding: 20px;">
                <!-- Image Container -->
                <div class="image-container" style="flex: 0 0 250px; display: flex; justify-content: center;">
                    <img src="images/MPGN.png" width="320" height="180" alt="Image">
                </div>

                <!-- Text Content Container -->
                <div class="text-container" style="flex: 1; max-width: 600px;">
                    <p class="papertitle" style="font-weight: bold; font-size: 1.0em;">
                        Modal-specific Pseudo Query Generation for Video Corpus Moment Retrieval
                    </p>
                    <p>
                        <u>Minjoon Jung</u>,
                        Seongho Choi,
                        <a href="https://tikatoka.github.io/">Joochan Kim</a>,
                        <a href="http://wityworks.com/">Jin-Hwa Kim*</a>,
                        <a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang*</a>
                    </p>
                    <p style="font-style: italic;">
                        Empirical Methods in Natural Language Processing (EMNLP), 2022 <br>
                        <a href="https://aclanthology.org/2022.emnlp-main.530/">paper</a> /
                        <a href="https://github.com/minjoong507/MPGN">code</a>
                    </p>
                    <p style="font-size: 12px">
                        We propose Modal-specific Pseudo Query Generation Network (MPGN), a self-supervised framework for Video Corpus Moment Retrieval (VCMR). MPGN captures orthogonal axis of information in videos and generates pseudo-queries that provide a considerable performance boost, even without human annotations.

                    </p>
                </div>
            </div>

            <div style="display: flex; align-items: flex-start; gap: 20px; padding: 20px;">
                <!-- Image Container -->
                <div class="image-container" style="flex: 0 0 250px; display: flex; justify-content: center;">
                    <img src="images/crosscutting.gif" width="320" height="180" alt="Animated GIF">
                </div>

                <!-- Text Content Container -->
                <div class="text-container" style="flex: 1; max-width: 600px;">
                    <p class="papertitle" style="font-weight: bold; font-size: 1.0em;">
                        Stagemix Video Generation using Face and Body Keypoints Detection
                    </p>
                    <p>
                        <u>Minjoon Jung</u>,
                        <a href="https://lsh3163.github.io/seunghyunlee/">Seung-Hyun Lee</a>,
                        Eunseon Sim,
                        Minho Jo,
                        Yujin Lee,
                        Hyebin Choi,
                        <a href="https://sites.google.com/view/cau-cvml/cvmlcau/junseokkwon?authuser=0">Junseok Kwon*</a>
                    </p>
                    <p style="font-style: italic;">
                        Multimedia Tools and Applications (MTAP), 2022 <br>
                        <a href="https://link.springer.com/article/10.1007/s11042-022-13103-8">paper</a> /
                        <a href="https://github.com/datamarket-tobigs/Cross-Cutting">code</a>
                    </p>
                    <p style="font-size: 12px">
                        We design a method for automatically creating Stagemix videos, which seamlessly combine multiple stage performances of a singer into a single cohesive video. We effectively produces natural-looking Stagemix videos while significantly reducing the effort involved compared to manual editing.
                    </p>
                </div>
            </div>
        </div>


        <!-- All Papers -->
        <div id="papers-all" class="tab-content">
            <div style="display: flex; align-items: flex-start; gap: 20px; padding: 20px;">
                <!-- Image Container -->
                <div class="image-container" style="flex: 0 0 250px; display: flex; justify-content: center;">
                    <img src="images/C2R.png" width="320" height="180">
                </div>
                <!-- Text Content Container -->
                <div class="text-container" style="flex: 1; max-width: 600px;">
                    <p class="papertitle" style="font-weight: bold; font-size: 1.0em;">
                        Confidence-guided Refinement Reasoning for Zero-shot Question Answering
                    </p>
                    <p>
                        Youwon Jang,
                        <u>Minjoon Jung</u>,
                        Woo-Suk Choi,
                        Minsoo Lee,
                        <a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang</a>,
                    </p>
                    <p style="font-style: italic;">
                        Empirical Methods in Natural Language Processing (EMNLP), 2025
                        <br>
                        </span>
                        <br>
                        TBD
                        <!--                        <a href="https://arxiv.org/abs/2411.12951">paper</a> /-->
                        <!--                        <a href="https://github.com/minjoong507/Consistency-of-Video-LLM">code</a>-->
                    </p>
                    <p style="font-size: 12px">
                        We introduce Confidence-guided Refinement Reasoning (C2R), a training-free framework that improves QA across text, image, and video domains by generating and refining sub-questions and answers, and then selecting the most reliable final answer based on confidence scores. The approach is flexible, model-agnostic, and shows consistent improvements across benchmarks.
                    </p>
                </div>
            </div>

            <div style="display: flex; align-items: flex-start; gap: 20px; padding: 20px;">
                <!-- Image Container -->
                <div class="image-container" style="flex: 0 0 250px; display: flex; justify-content: center;">
                    <img src="images/egoexo.png" width="320" height="220">
                </div>

                <!-- Text Content Container -->
                <div class="text-container" style="flex: 1; max-width: 600px;">
                    <p class="papertitle" style="font-weight: bold; font-size: 1.0em;">
                        Towards Video Large Langauge Models for View-Invariant Video Understanding
                    </p>
                    <p>
                        <u>Minjoon Jung</u>,
                        <a href="https://doc-doc.github.io/cv/">Junbin Xiao*</a>,
                        <a href="https://jhkim-snu.github.io/">Junghyun Kim</a>,
                        Il-Jae Kwon,
                        <a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang</a>,
                        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a>
                    </p>
                    <p style="font-style: italic;">
                        Technical Report <br>
                        </span>
                        <br>
                        TBD
                        <!--                        <a href="https://arxiv.org/abs/2411.12951">paper</a> /-->
                        <!--                        <a href="https://github.com/minjoong507/Consistency-of-Video-LLM">code</a>-->
                    </p>
                    <p style="font-size: 12px">
                        We introduce EgoExo-Con, a new benchmark comprising synchronized ego-exo video with human-refined queries, to study view-invariant video understanding in Video-LLMs. We also propose a scalable self-guided learning method that achieves consistent temporal comprehension with comparable performance to state-of-the-art models.
                    </p>
                </div>
            </div>


            <div style="display: flex; align-items: flex-start; gap: 20px; padding: 20px;">
                <!-- Image Container -->
                <div class="image-container" style="flex: 0 0 250px; display: flex; justify-content: center;">
                    <img src="images/consistency.gif" width="320" height="180" alt="Animated GIF">
                </div>

                <!-- Text Content Container -->
                <div class="text-container" style="flex: 1; max-width: 600px;">
                    <p class="papertitle" style="font-weight: bold; font-size: 1.0em;">
                        On the Consistency of Video Large Language Models in Temporal Comprehension
                    </p>
                    <p>
                        <u>Minjoon Jung</u>,
                        <a href="https://doc-doc.github.io/cv/">Junbin Xiao*</a>,
                        <a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang</a>,
                        <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a>
                    </p>
                    <p style="font-style: italic;">
                        Conference on Computer Vision and Pattern Recognition (CVPR), 2025 <br>
                        <span style="font-size: smaller;">
                      *Earlier version has been accepted by
                      <a href="https://neurips.cc/virtual/2024/103565" style="font-size: inherit;">NeurIPS 2024 Workshop on Video-Language Models</a>.
                    </span>
                        <br>
                        <a href="https://arxiv.org/abs/2411.12951">paper</a> /
                        <a href="https://github.com/minjoong507/Consistency-of-Video-LLM">code</a>
                    </p>
                    <p style="font-size: 12px">
                        We reveal that Video Large Language Models (Video-LLMs) struggle to maintain consistency in grounding and verification. We systematically analyze this issue and introduce VTune, an effective instruction tuning method, leading to substantial improvements in both grounding and consistency.
                    </p>
                </div>
            </div>

            <div style="display: flex; align-items: flex-start; gap: 20px; padding: 20px;">
                <!-- Image Container -->
                <div class="image-container" style="flex: 0 0 250px; display: flex; justify-content: center;">
                    <img src="images/ordinal.jpg" width="320" height="160">
                </div>

                <!-- Text Content Container -->
                <div class="text-container" style="flex: 1; max-width: 600px;">
                    <p class="papertitle" style="font-weight: bold; font-size: 1.0em;">
                        Exploring Ordinal Bias in Action Recognition for Instructional Videos
                    </p>
                    <p>
                        <a href="https://tikatoka.github.io/">Joochan Kim</a>,
                        <u>Minjoon Jung</u>,
                        <a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang*</a>
                    </p>
                    <p style="font-style: italic;">
                        ICLR Workshop on Spurious Correlation and Shortcut Learning: Foundations and Solutions, 2025 <br>
                        <span style="font-size: smaller;">
                    </span>
                        <br>
                        <a href="https://openreview.net/forum?id=JIon6Xuvm1&referrer=%5Bthe%20profile%20of%20Joochan%20Kim%5D(%2Fprofile%3Fid%3D~Joochan_Kim1)">paper</a> /
                        <a>code</a>
                    </p>
                    <p style="font-size: 12px">
                        We study that ordinal bias leads action recognition models to over-rely on dominant action pairs, inflating performance and lacking true video comprehension.
                    </p>
                </div>
            </div>

            <div style="display: flex; align-items: flex-start; gap: 20px; padding: 20px;">
                <!-- Image Container -->
                <div class="image-container" style="flex: 0 0 250px; display: flex; justify-content: center;">
                    <img src="images/bm-detr.png" width="320" height="180" alt="Image">
                </div>

                <!-- Text Content Container -->
                <div class="text-container" style="flex: 1; max-width: 600px;">
                    <p class="papertitle" style="font-weight: bold; font-size: 1.0em;">
                        Background-aware Moment Detection for Video Moment Retrieval
                    </p>
                    <p>
                        <u>Minjoon Jung</u>,
                        Youwon Jang,
                        Seongho Choi,
                        <a href="https://tikatoka.github.io/">Joochan Kim</a>,
                        <a href="http://wityworks.com/">Jin-Hwa Kim*</a>,
                        <a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang*</a>
                    </p>
                    <p style="font-style: italic;">
                        Winter Conference on Applications of Computer Vision (WACV), 2025 <br>
                        <span style="font-size: smaller;">*Early accepted in Round 1.</span> <br>
                        <a href="https://arxiv.org/abs/2306.02728">paper</a> /
                        <a href="https://github.com/minjoong507/BM-DETR">code</a>
                    </p>
                    <p style="font-size: 12px">
                        We propose Background-aware Moment Detection TRansformer (BM-DETR), which carefully adopts a contrastive approach for robust prediction. BM-DETR achieves state-of-the-art performance on various benchmarks while being highly efficient.
                    </p>
                </div>
            </div>

            <div style="display: flex; align-items: flex-start; gap: 20px; padding: 20px;">
                <!-- Image Container -->
                <!--                <div class="image-container" style="flex: 0 0 250px; display: flex; justify-content: center;">-->
                <!--                    <img src="images/PGA.png" width="320" height="180" alt="Image">-->
                <!--                </div>-->
                <div class="image-container" style="flex: 0 0 250px; display: flex; justify-content: center;">
                    <img src="images/IROS_demo.gif" width="320" height="180" alt="Animated GIF">
                </div>

                <!-- Text Content Container -->
                <div class="text-container" style="flex: 1; max-width: 600px;">
                    <p class="papertitle" style="font-weight: bold; font-size: 1.0em;">
                        PGA: Personalizing Grasping Agents with Single Human-Robot Interaction
                    </p>
                    <p>
                        <a href="https://jhkim-snu.github.io/">Junghyun Kim</a>,
                        <a href="https://gicheonkang.com/">Gi-Cheon Kang</a>,
                        Jaein Kim, Seoyun Yang,
                        <u>Minjoon Jung</u>,
                        <a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang*</a>
                    </p>
                    <p style="font-style: italic;">
                        International Conference on Intelligent Robots and Systems (IROS), 2024 <span style="color: red; font-weight: bold;">(Oral)</span> <br>
                        <a href="https://arxiv.org/abs/2310.12547">paper</a> /
                        <a href="https://github.com/JHKim-snu/PGA">code</a>
                    </p>
                    <p style="font-size: 12px">
                        We propose Personalized Grasping Agent (PGA), which enables robots to grasp user-specific objects from just a single interaction. PGA captures multi-view object data and uses label propagation to adapt its grasping model without requiring extensive annotations, achieving performance close to fully supervised methods.
                    </p>
                </div>
            </div>

            <div style="display: flex; align-items: flex-start; gap: 20px; padding: 20px;">
                <!-- Image Container -->
                <div class="image-container" style="flex: 0 0 250px; display: flex; justify-content: center;">
                    <img src="images/MPGN.png" width="320" height="180" alt="Image">
                </div>

                <!-- Text Content Container -->
                <div class="text-container" style="flex: 1; max-width: 600px;">
                    <p class="papertitle" style="font-weight: bold; font-size: 1.0em;">
                        Modal-specific Pseudo Query Generation for Video Corpus Moment Retrieval
                    </p>
                    <p>
                        <u>Minjoon Jung</u>,
                        Seongho Choi,
                        <a href="https://tikatoka.github.io/">Joochan Kim</a>,
                        <a href="http://wityworks.com/">Jin-Hwa Kim*</a>,
                        <a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang*</a>
                    </p>
                    <p style="font-style: italic;">
                        Empirical Methods in Natural Language Processing (EMNLP), 2022 <br>
                        <a href="https://aclanthology.org/2022.emnlp-main.530/">paper</a> /
                        <a href="https://github.com/minjoong507/MPGN">code</a>
                    </p>
                    <p style="font-size: 12px">
                        We propose Modal-specific Pseudo Query Generation Network (MPGN), a self-supervised framework for Video Corpus Moment Retrieval (VCMR). MPGN captures orthogonal axis of information in videos and generates pseudo-queries that provide a considerable performance boost, even without human annotations.

                    </p>
                </div>
            </div>

            <div style="display: flex; align-items: flex-start; gap: 20px; padding: 20px;">
                <!-- Image Container -->
                <div class="image-container" style="flex: 0 0 250px; display: flex; justify-content: center;">
                    <img src="images/crosscutting.gif" width="320" height="180" alt="Animated GIF">
                </div>

                <!-- Text Content Container -->
                <div class="text-container" style="flex: 1; max-width: 600px;">
                    <p class="papertitle" style="font-weight: bold; font-size: 1.0em;">
                        Stagemix Video Generation using Face and Body Keypoints Detection
                    </p>
                    <p>
                        <u>Minjoon Jung</u>,
                        <a href="https://lsh3163.github.io/seunghyunlee/">Seung-Hyun Lee</a>,
                        Eunseon Sim,
                        Minho Jo,
                        Yujin Lee,
                        Hyebin Choi,
                        <a href="https://sites.google.com/view/cau-cvml/cvmlcau/junseokkwon?authuser=0">Junseok Kwon*</a>
                    </p>
                    <p style="font-style: italic;">
                        Multimedia Tools and Applications (MTAP), 2022 <br>
                        <a href="https://link.springer.com/article/10.1007/s11042-022-13103-8">paper</a> /
                        <a href="https://github.com/datamarket-tobigs/Cross-Cutting">code</a>
                    </p>
                    <p style="font-size: 12px">
                        We design a method for automatically creating Stagemix videos, which seamlessly combine multiple stage performances of a singer into a single cohesive video. We effectively produces natural-looking Stagemix videos while significantly reducing the effort involved compared to manual editing.
                    </p>
                </div>
            </div>
            <div style="display: flex; align-items: flex-start; gap: 20px; padding: 20px;">
                <!-- Image Container -->
                <div class="image-container" style="flex: 0 0 250px; display: flex; justify-content: center;">
                    <img src="images/vtt.png" width="320" height="180" alt="Image">
                </div>

                <!-- Text Content Container -->
                <div class="text-container" style="flex: 1; max-width: 600px;">
                    <p class="papertitle" style="font-weight: bold; font-size: 1.0em;">
                        Toward a Human-Level Video Understanding Intelligence
                    </p>
                    <p>
                        <a href="https://yujungheo.github.io/">Yu-Jung Heo,</a>
                        Minsu Lee,
                        Seongho Choi,
                        Woo Suk Choi,
                        Minjung Shin,
                        <u>Minjoon Jung</u>,
                        Jeh-Kwang Ryu,
                        <a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang*</a>
                    </p>
                    <p style="font-style: italic;">
                        AAAI Fall Symposium Series on Artificial Intelligence for Human-Robot Interaction, 2021<br>
                        <a href="https://arxiv.org/abs/2110.04203">paper</a>
                    </p>
                    <p style="font-size: 12px">
                        We aim to develop an AI agent that can watch video clips and have a conversation with human about the video story.
                    </p>
                </div>
            </div>
        </div>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <p align="center">
            <a href="https://clustrmaps.com/site/1bxs5"  title="Visit tracker">
                <img src="//www.clustrmaps.com/map_v2.png?d=FtUdLRtU5_IUma_g6SQ6pfZabTo-l09PRKkLe52CeyU&cl=ffffff"></a>
        </p>
        <tr>
            <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                    <a href="https://minjoong507.github.io/">Minjoon Jung</a>. <a href="https://bi.snu.ac.kr/">BI LAB</a>, <a href="https://en.snu.ac.kr/">Seoul National University</a>
                </p>
            </td>
        </tr>
        </tbody></table>
    </td>
</tr>
</table>
</body>

</html>
